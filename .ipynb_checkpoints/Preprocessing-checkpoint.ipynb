{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "grand-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import pointbiserialr, kruskal, ttest_ind, f_oneway, shapiro, mannwhitneyu, levene\n",
    "# !pip install pingouin\n",
    "# import pingouin as pg\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# !pip install category_encoders\n",
    "from category_encoders import WOEEncoder, BaseNEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-washington",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatal-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/Train.csv', index_col=[0])\n",
    "test = pd.read_csv('Data/Test_fin.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-dynamics",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elder-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransactions(rem_act_no, bene_act_no):\n",
    "    return train[(train['rem_act_no_dummy']==rem_act_no) & (train['bene_act_no_dummy']==bene_act_no)]\n",
    "\n",
    "\n",
    "def getGapVar(x):\n",
    "    if len(x)>1:\n",
    "        return x['transaction_val_dt'].sort_values().diff()[1:].apply(lambda x: x.days).var(ddof=0)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def getAvgTxnPerDay(x):\n",
    "    return len(x)/x['transaction_val_dt'].nunique()\n",
    "\n",
    "\n",
    "def ddProps(x):\n",
    "    days = x.apply(lambda x: x.day)\n",
    "    return np.mean(days), np.var(days, ddof=0) \n",
    "\n",
    "\n",
    "def countNumInd(x):\n",
    "    n_inds = len(np.unique(x))\n",
    "    if n_inds == 1 or 'Unknown' not in x:\n",
    "        return n_inds\n",
    "    else:\n",
    "        return n_inds-1\n",
    "    \n",
    "def fn(x):\n",
    "    if len(x)>1 and 'Unknown' in x:\n",
    "        x = list(x)\n",
    "        x.remove('Unknown')\n",
    "    return x[0]\n",
    "\n",
    "def get_n_ind(x, multi_ind_comp, multi_ind_rem, multi_ind_bene):\n",
    "    comp = x['rem_company_id_dummy']\n",
    "    rem = x['rem_act_no_dummy']\n",
    "    bene = x['bene_act_no_dummy']\n",
    "    return multi_ind_comp[comp], multi_ind_rem[rem], multi_ind_bene[bene]\n",
    "\n",
    "\n",
    "def get_dd_global_props(x, comp_dd_means, comp_dd_vars, rem_dd_means, rem_dd_vars, bene_dd_means, bene_dd_vars):\n",
    "    comp = x['rem_company_id_dummy']\n",
    "    rem = x['rem_act_no_dummy']\n",
    "    bene = x['bene_act_no_dummy']\n",
    "    return abs(comp_dd_means[comp]-x['dd_mean']), x['dd_var']/comp_dd_vars[comp], abs(rem_dd_means[rem]-x['dd_mean']), x['dd_var']/rem_dd_vars[rem], abs(bene_dd_means[bene]-x['dd_mean']), x['dd_var']/bene_dd_vars[bene]\n",
    "\n",
    "def get_dd_props_txn(x, comp_dd_means, rem_dd_means, bene_dd_means):\n",
    "    comp = x['rem_company_id_dummy']\n",
    "    rem = x['rem_act_no_dummy']\n",
    "    bene = x['bene_act_no_dummy']\n",
    "    return abs(comp_dd_means[comp]-x['dd']), abs(rem_dd_means[rem]-x['dd']), abs(bene_dd_means[bene]-x['dd'])\n",
    "\n",
    "def get_txn_props(x, comp_txn_means, comp_txn_vars, rem_txn_means, rem_txn_vars, bene_txn_means, bene_txn_vars):\n",
    "    comp = x['rem_company_id_dummy']\n",
    "    rem = x['rem_act_no_dummy']\n",
    "    bene = x['bene_act_no_dummy']\n",
    "    return comp_txn_means[comp]-x['txn_amt_mean'], x['txn_amt_var']/comp_txn_vars[comp], rem_txn_means[rem]-x['txn_amt_mean'], x['txn_amt_var']/rem_txn_vars[rem], bene_txn_means[bene]-x['txn_amt_mean'], x['txn_amt_var']/bene_txn_vars[bene]\n",
    "\n",
    "def get_txn_props_txn(x, comp_txn_means, rem_txn_means, bene_txn_means):\n",
    "    comp = x['rem_company_id_dummy']\n",
    "    rem = x['rem_act_no_dummy']\n",
    "    bene = x['bene_act_no_dummy']\n",
    "    return comp_txn_means[comp]-x['txn_amt'], rem_txn_means[rem]-x['txn_amt'], bene_txn_means[bene]-x['txn_amt']\n",
    "\n",
    "def get_txn_per_day_ratio(x, avgTxnPerDayComp, avgTxnPerDayRem, avgTxnPerDayBene):\n",
    "    comp = x['rem_company_id_dummy']\n",
    "    rem = x['rem_act_no_dummy']\n",
    "    bene = x['bene_act_no_dummy']\n",
    "    return x['txn_per_day_mean']/avgTxnPerDayComp[comp], x['txn_per_day_mean']/avgTxnPerDayRem[rem], x['txn_per_day_mean']/avgTxnPerDayBene[bene]\n",
    "\n",
    "\n",
    "def get_txn_gaps(x, avgTxnGapComp, varTxnGapComp, avgTxnGapRem, varTxnGapRem, avgTxnGapBene, varTxnGapBene):\n",
    "    comp = x['rem_company_id_dummy']\n",
    "    rem = x['rem_act_no_dummy']\n",
    "    bene = x['bene_act_no_dummy']\n",
    "    return avgTxnGapComp[comp]-x['txn_gap_mean'], x['txn_gap_var']/varTxnGapComp[comp], avgTxnGapRem[rem]-x['txn_gap_mean'], x['txn_gap_var']/varTxnGapRem[rem], avgTxnGapBene[bene]-x['txn_gap_mean'], x['txn_gap_var']/varTxnGapBene[bene]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-rally",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "respected-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, date_path, baseEnc_path, woeEnc_path, train=True):\n",
    "    \n",
    "    ########################## Cleaning ###########################\n",
    "        \n",
    "    # Outlier removal\n",
    "    if train:\n",
    "        print('Fraction of rows removed:', sum(data['txn_amt']>100)/len(data))\n",
    "        data = data[data['txn_amt']<=100]\n",
    "    else:\n",
    "        data.loc[data['txn_amt']>100, 'txn_amt'] = 100\n",
    "\n",
    "    # Encode dates\n",
    "    trans_date = data['transaction_val_dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "    data['transaction_val_dt'] = trans_date\n",
    "\n",
    "    # Add dd column\n",
    "    data['dd'] = data.apply(lambda x: x['transaction_val_dt'].day, axis=1)\n",
    "\n",
    "    ########################## Paired Data #########################\n",
    "\n",
    "    # Get pair groups\n",
    "    rem_bene = data.groupby(['rem_act_no_dummy', 'bene_act_no_dummy'])\n",
    "    rem_bene_groups = rem_bene.groups\n",
    "\n",
    "    ###################### Features of paired data ##################\n",
    "\n",
    "    ###################### 1. Stand Alone Features ##################\n",
    "\n",
    "    print('Stand alone features')\n",
    "\n",
    "    # column: n_txn\n",
    "    counts = rem_bene.count()['txn_amt']\n",
    "    if train == True:\n",
    "        # column: pay_roll_ind\n",
    "        sol = rem_bene.mean()['payroll_ind'].values.reshape(-1, 1)\n",
    "        groupDF = pd.DataFrame(data=np.append(counts.values.reshape(-1, 1), sol, axis=1), index=counts.index.to_flat_index(), columns=['n_txn', 'payroll_ind'])\n",
    "        groupDF = groupDF.astype({'n_txn':int, 'payroll_ind':int})\n",
    "    else:\n",
    "        groupDF = pd.DataFrame(data=counts.values.reshape(-1, 1), index=counts.index.to_flat_index(), columns=['n_txn'])\n",
    "        groupDF = groupDF.astype({'n_txn':int})\n",
    "\n",
    "    # column: rem_company_id_dummy, rem_act_no_dummy, bene_act_no_dummy (remove later)\n",
    "    groupDF['rem_company_id_dummy'] = rem_bene['rem_company_id_dummy'].first().values\n",
    "    groupDF['rem_act_no_dummy'], groupDF['bene_act_no_dummy'] = groupDF.index.str\n",
    "\n",
    "    # column: txn_amt_var, txn_amt_mean\n",
    "    groupDF['txn_amt_var'] = rem_bene['txn_amt'].var(ddof=0)\n",
    "    groupDF['txn_amt_mean'] = rem_bene['txn_amt'].mean()\n",
    "\n",
    "    # column: txn_rate, txn_gap_mean\n",
    "    delta = (rem_bene['transaction_val_dt'].max()-rem_bene['transaction_val_dt'].min()).apply(lambda x: x.days)\n",
    "    groupDF['txn_rate'] = groupDF['n_txn']/delta\n",
    "    groupDF['txn_gap_mean'] = delta/(groupDF['n_txn']-1)\n",
    "    groupDF.loc[groupDF['txn_rate'] == np.inf, 'txn_rate'] = np.nan\n",
    "    # Fill na with means\n",
    "    groupDF = groupDF.fillna(groupDF.mean())\n",
    "\n",
    "    # column: txn_gap_var\n",
    "    varTxnGap = rem_bene.apply(getGapVar)\n",
    "    groupDF['txn_gap_var'] = varTxnGap\n",
    "\n",
    "    # column: txn_per_day_mean\n",
    "    avgTxnPerDay = rem_bene.apply(getAvgTxnPerDay)\n",
    "    groupDF['txn_per_day_mean'] = avgTxnPerDay\n",
    "\n",
    "    # column: dd_mean, dd_var\n",
    "    day_stats = rem_bene['transaction_val_dt'].apply(ddProps)\n",
    "    groupDF['dd_mean'], groupDF['dd_var'] = day_stats.str\n",
    "\n",
    "    ####################### 2. Global Features ##################\n",
    "\n",
    "    print('Global features')\n",
    "\n",
    "    comp_gps = data.groupby('rem_company_id_dummy')\n",
    "    rem_gps = data.groupby('rem_act_no_dummy')\n",
    "    bene_gps = data.groupby('bene_act_no_dummy')\n",
    "\n",
    "    # column: xxxx_n_ind\n",
    "    multi_ind_comp = comp_gps['rem_company_ind'].unique().apply(countNumInd)\n",
    "    multi_ind_rem = rem_gps['rem_company_ind'].unique().apply(countNumInd)      \n",
    "    multi_ind_bene = bene_gps['rem_company_ind'].unique().apply(countNumInd)\n",
    "\n",
    "    n_ind = groupDF.apply(get_n_ind, axis=1, multi_ind_comp=multi_ind_comp, multi_ind_rem=multi_ind_rem, multi_ind_bene=multi_ind_bene)\n",
    "    groupDF['comp_n_ind'], groupDF['rem_n_ind'], groupDF['bene_n_ind'] = n_ind.str\n",
    "\n",
    "    # column: xxxx_dd_diff, xxxx_dd_var_ratio\n",
    "    comp_dd_means = comp_gps['dd'].mean()\n",
    "    comp_dd_vars = comp_gps['dd'].var(ddof=0)\n",
    "    rem_dd_means = rem_gps['dd'].mean()\n",
    "    rem_dd_vars = rem_gps['dd'].var(ddof=0)\n",
    "    bene_dd_means = bene_gps['dd'].mean()\n",
    "    bene_dd_vars = bene_gps['dd'].var(ddof=0)    \n",
    "\n",
    "    dd_props = groupDF.apply(get_dd_global_props, axis=1, comp_dd_means=comp_dd_means, comp_dd_vars=comp_dd_vars, rem_dd_means=rem_dd_means, rem_dd_vars=rem_dd_vars, bene_dd_means=bene_dd_means, bene_dd_vars=bene_dd_vars)\n",
    "    groupDF['comp_dd_diff'], groupDF['comp_dd_var_ratio'], groupDF['rem_dd_diff'], groupDF['rem_dd_var_ratio'], groupDF['bene_dd_diff'], groupDF['bene_dd_var_ratio']  = dd_props.str\n",
    "\n",
    "    # column: xxxx_txn_diff, xxxx_txn_var_ratio\n",
    "    comp_txn_means = comp_gps['txn_amt'].mean()\n",
    "    comp_txn_vars = comp_gps['txn_amt'].var(ddof=0)\n",
    "    rem_txn_means = rem_gps['txn_amt'].mean()\n",
    "    rem_txn_vars = rem_gps['txn_amt'].var(ddof=0)\n",
    "    bene_txn_means = bene_gps['txn_amt'].mean()\n",
    "    bene_txn_vars = bene_gps['txn_amt'].var(ddof=0)\n",
    "\n",
    "    txn_props = groupDF.apply(get_txn_props, axis=1, comp_txn_means=comp_txn_means, comp_txn_vars=comp_txn_vars, rem_txn_means=rem_txn_means, rem_txn_vars=rem_txn_vars, bene_txn_means=bene_txn_means, bene_txn_vars=bene_txn_vars)\n",
    "    groupDF['comp_txn_diff'], groupDF['comp_txn_var_ratio'], groupDF['rem_txn_diff'], groupDF['rem_txn_var_ratio'], groupDF['bene_txn_diff'], groupDF['bene_txn_var_ratio'] = txn_props.str\n",
    "\n",
    "    # column: xxxx_txn_per_day_ratio\n",
    "    avgTxnPerDayComp = comp_gps.apply(getAvgTxnPerDay)\n",
    "    avgTxnPerDayRem = rem_gps.apply(getAvgTxnPerDay)\n",
    "    avgTxnPerDayBene = bene_gps.apply(getAvgTxnPerDay)\n",
    "\n",
    "    txn_per_day_ratio = groupDF.apply(get_txn_per_day_ratio, axis=1, avgTxnPerDayComp=avgTxnPerDayComp, avgTxnPerDayRem=avgTxnPerDayRem, avgTxnPerDayBene=avgTxnPerDayBene)\n",
    "    groupDF['comp_txn_per_day_ratio'], groupDF['rem_txn_per_day_ratio'], groupDF['bene_txn_per_day_ratio'] = txn_per_day_ratio.str\n",
    "\n",
    "    # column: xxxx_txn_gap_diff, xxxx_txn_gap_var_ratio\n",
    "    deltaComp = (comp_gps['transaction_val_dt'].max()-comp_gps['transaction_val_dt'].min()).apply(lambda x: x.days)\n",
    "    avgTxnGapComp = deltaComp/(comp_gps.count()['txn_amt']-1)\n",
    "    varTxnGapComp = comp_gps.apply(getGapVar)\n",
    "\n",
    "    deltaRem = (rem_gps['transaction_val_dt'].max()-rem_gps['transaction_val_dt'].min()).apply(lambda x: x.days)\n",
    "    avgTxnGapRem = deltaRem/(rem_gps.count()['txn_amt']-1)\n",
    "    varTxnGapRem = rem_gps.apply(getGapVar)\n",
    "\n",
    "    deltaBene = (bene_gps['transaction_val_dt'].max()-bene_gps['transaction_val_dt'].min()).apply(lambda x: x.days)\n",
    "    avgTxnGapBene = deltaBene/(bene_gps.count()['txn_amt']-1)\n",
    "    varTxnGapBene = bene_gps.apply(getGapVar)    \n",
    "\n",
    "    txn_gap_info = groupDF.apply(get_txn_gaps, axis=1, avgTxnGapComp=avgTxnGapComp, varTxnGapComp=varTxnGapComp, avgTxnGapRem=avgTxnGapRem, varTxnGapRem=varTxnGapRem, avgTxnGapBene=avgTxnGapBene, varTxnGapBene=varTxnGapBene)\n",
    "    groupDF['comp_txn_gap_diff'], groupDF['comp_txn_gap_var_ratio'], groupDF['rem_txn_gap_diff'], groupDF['rem_txn_gap_var_ratio'], groupDF['bene_txn_gap_diff'], groupDF['bene_txn_gap_var_ratio'] = txn_gap_info.str\n",
    "\n",
    "    groupDF.drop(['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy'], axis=1, inplace=True)\n",
    "\n",
    "    ############### Transaction Data ######################\n",
    "\n",
    "    # column: xxxx_n_ind\n",
    "    n_ind_txn = data.apply(get_n_ind, axis=1, multi_ind_comp=multi_ind_comp, multi_ind_rem=multi_ind_rem, multi_ind_bene=multi_ind_bene)\n",
    "    data['comp_n_ind'], data['rem_n_ind'], data['bene_n_ind'] = n_ind_txn.str\n",
    "\n",
    "    # column: xxxx_dd_diff  \n",
    "    dd_props_txn = data.apply(get_dd_props_txn, axis=1, comp_dd_means=comp_dd_means, rem_dd_means=rem_dd_means, bene_dd_means=bene_dd_means)\n",
    "    data['comp_dd_diff'], data['rem_dd_diff'], data['bene_dd_diff'] = dd_props_txn.str\n",
    "\n",
    "    # column: xxxx_txn_diff\n",
    "    txn_props_txn = data.apply(get_txn_props_txn, axis=1, comp_txn_means=comp_txn_means, rem_txn_means=rem_txn_means, bene_txn_means=bene_txn_means)\n",
    "    data['comp_txn_diff'], data['rem_txn_diff'], data['bene_txn_diff'] = txn_props_txn.str\n",
    "\n",
    "    # Encoding\n",
    "    if train:\n",
    "        # Date \n",
    "        if os.path.exists(date_path): \n",
    "            min_date = pickle.load(open(date_path, 'rb'))\n",
    "        else:    \n",
    "            min_date = min(data['transaction_val_dt'])\n",
    "            pickle.dump(min_date, open(date_path, 'wb'))\n",
    "\n",
    "        data['time'] = (data['transaction_val_dt']-min_date).apply(lambda x: x.days)\n",
    "        data.replace({'DOMESTIC':1,'CROSS BORDER':0}, inplace=True)\n",
    "\n",
    "        if os.path.exists(baseEnc_path):\n",
    "            enc1 = pickle.load(open(baseEnc_path, 'rb'))\n",
    "        else:\n",
    "            enc1 = BaseNEncoder(cols=['rem_company_ind'], base=2).fit(data['rem_company_ind'])\n",
    "            pickle.dump(enc1, open(baseEnc_path, 'wb'))\n",
    "\n",
    "#         if os.path.exists(woeEnc_path):\n",
    "#             enc2 = pickle.load(open(woeEnc_path, 'rb'))\n",
    "#         else:\n",
    "#             X = data[['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy']]\n",
    "#             y = data['payroll_ind']\n",
    "#             enc2 = WOEEncoder(cols=['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy']).fit(X, y)\n",
    "#             pickle.dump(enc2, open(woeEnc_path, 'wb'))\n",
    "\n",
    "        encoded = enc1.transform(data['rem_company_ind'])\n",
    "        data = pd.merge(data, encoded, left_index=True, right_index=True)\n",
    "#         data[['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy']] = enc2.transform(data[['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy']])\n",
    "        data.drop(['transaction_val_dt', 'yearmonth', 'rem_company_ind'], axis=1, inplace=True)\n",
    "    else:\n",
    "        if os.path.exists(date_path): \n",
    "            min_date = pickle.load(open(date_path, 'rb'))\n",
    "        else:\n",
    "            print('Date file not found!')\n",
    "\n",
    "        data['time'] = (data['transaction_val_dt']-min_date).apply(lambda x: x.days)\n",
    "        data.replace({'DOMESTIC':1,'CROSS BORDER':0}, inplace=True)\n",
    "\n",
    "        if os.path.exists(baseEnc_path):\n",
    "            enc1 = pickle.load(open(baseEnc_path, 'rb'))\n",
    "        else:\n",
    "            print('BaseEnc file not found!')\n",
    "\n",
    "#         if os.path.exists(woeEnc_path):\n",
    "#             enc2 = pickle.load(open(woeEnc_path, 'rb'))\n",
    "#         else:\n",
    "#             print('WOEEnc file not found!')\n",
    "\n",
    "        encoded = enc1.transform(data['rem_company_ind'])\n",
    "        data = pd.merge(data, encoded, left_index=True, right_index=True)\n",
    "#         data[['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy']] = enc2.transform(data[['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy']])\n",
    "        data.drop(['rem_plus_bene', 'transaction_val_dt', 'yearmonth', 'rem_company_ind'], axis=1, inplace=True)\n",
    "        data = data.drop(['rem_n_ind','rem_dd_diff','rem_txn_diff', 'rem_company_ind_0'], axis=1)\n",
    "        data = data.drop(['rem_company_id_dummy', 'rem_act_no_dummy', 'bene_act_no_dummy'], axis=1)\n",
    "        \n",
    "    return groupDF, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "terminal-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_featues_pair_data(groupDF, data, company_enc_path, baseEnc_pair_path, imputer_path, train=True):\n",
    "    '''\n",
    "    groupDF : Obtained from Preprocess function\n",
    "    data    : raw test/train data\n",
    "    \n",
    "    ## change line number 15 to commented version if package is updated!!!!!!!!!!\n",
    "    \n",
    "    '''\n",
    "    # txn_type_mean\n",
    "    data['rem_plus_bene'] = data.apply(lambda x: (x.rem_act_no_dummy, x.bene_act_no_dummy), axis=1)\n",
    "    data.replace({'DOMESTIC':1,'CROSS BORDER':0},inplace=True)\n",
    "    rem_bene = data.groupby(['rem_act_no_dummy', 'bene_act_no_dummy'])\n",
    "    groupDF['txn_type_mean'] = rem_bene['txn_type'].mean()\n",
    "    \n",
    "    # company_ind\n",
    "    groupDF['rem_company_ind'] = rem_bene['rem_company_ind'].unique().apply(fn)  #########.apply(lambda x: statistics.mode(x))\n",
    "    \n",
    "    # company id\n",
    "#     groupDF['rem_company_id_dummy'] = rem_bene['rem_company_id_dummy'].first()\n",
    "    \n",
    "    if train:\n",
    "#         # company id encoder \n",
    "#         if os.path.exists(company_enc_path):\n",
    "#             enc = pickle.load(open(company_enc_path, 'rb'))\n",
    "#         else:\n",
    "#             X = groupDF['rem_company_id_dummy']\n",
    "#             y = groupDF['payroll_ind']\n",
    "#             enc = WOEEncoder(cols=['rem_company_id_dummy']).fit(X, y)\n",
    "#             pickle.dump(enc, open(company_enc_path, 'wb'))    \n",
    "\n",
    "        # ind encoder\n",
    "        if os.path.exists(baseEnc_pair_path):\n",
    "            enc1 = pickle.load(open(baseEnc_pair_path, 'rb'))\n",
    "        else:\n",
    "            enc1 = BaseNEncoder(cols=['rem_company_ind'], base=2).fit(groupDF['rem_company_ind'])\n",
    "            pickle.dump(enc1, open(baseEnc_pair_path, 'wb'))            \n",
    "    else:\n",
    "#         if os.path.exists(company_enc_path):\n",
    "#             enc = pickle.load(open(company_enc_path, 'rb'))\n",
    "#         else:\n",
    "#             print('WOE Encoder not found!')\n",
    "            \n",
    "        if os.path.exists(baseEnc_pair_path):\n",
    "            enc1 = pickle.load(open(baseEnc_pair_path, 'rb'))\n",
    "        else:\n",
    "            enc1 = BaseNEncoder(cols=['rem_company_ind'], base=2).fit(groupDF['rem_company_ind'])\n",
    "            print('Binary Encoder not found!')           \n",
    "                \n",
    "#     groupDF['rem_company_id_dummy'] = enc.transform(groupDF['rem_company_id_dummy'])\n",
    "    encoded = enc1.transform(groupDF['rem_company_ind'])\n",
    "    groupDF = pd.merge(groupDF, encoded, left_index=True, right_index=True)\n",
    "    groupDF.drop(['rem_company_ind'], axis=1, inplace=True)\n",
    "    ###### Dropping since std dev of this column was found to be zero!\n",
    "    groupDF.drop(['rem_company_ind_0'], axis=1, inplace=True)\n",
    "    \n",
    "    # Conver inf to nan\n",
    "    groupDF.replace({np.inf: np.nan}, inplace=True)\n",
    "    \n",
    "    # Columns with too many NaNs\n",
    "    c_drop = ['bene_dd_var_ratio', 'bene_txn_var_ratio', 'bene_txn_gap_diff', 'bene_txn_gap_var_ratio']\n",
    "    groupDF.drop(c_drop, axis=1, inplace=True)\n",
    "    \n",
    "    if train:\n",
    "        if os.path.exists(imputer_path):\n",
    "            imputer  = pickle.load(open(imputer_path, 'rb'))\n",
    "        else:\n",
    "            imputer = KNNImputer(n_neighbors=5).fit(groupDF.drop('payroll_ind', axis=1))\n",
    "            pickle.dump(imputer, open(imputer_path, 'wb'))\n",
    "        temp = imputer.transform(groupDF.drop('payroll_ind', axis=1))\n",
    "        temp = pd.DataFrame(temp, columns=groupDF.drop('payroll_ind', axis=1).columns, index=groupDF.index)\n",
    "        temp['payroll_ind'] = groupDF['payroll_ind']\n",
    "        groupDF = temp\n",
    "    else:\n",
    "        if os.path.exists(imputer_path):\n",
    "            imputer  = pickle.load(open(imputer_path, 'rb'))\n",
    "        else:\n",
    "            print('Imputer does not exist')\n",
    "            return\n",
    "        temp = imputer.transform(groupDF)\n",
    "        groupDF = pd.DataFrame(temp, columns=groupDF.columns, index=groupDF.index)\n",
    "\n",
    "    depCols = ['rem_txn_gap_diff', 'rem_dd_var_ratio', 'rem_n_ind', 'rem_dd_diff', 'rem_txn_diff']\n",
    "    groupDF.drop(depCols, axis=1, inplace=True)\n",
    "    \n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "virtual-transport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of rows removed: 0.004035975683491154\n",
      "Stand alone features\n",
      "Global features\n"
     ]
    }
   ],
   "source": [
    "groupDF, train_data = preprocess(train, date_path = 'Data/min_date.sav', baseEnc_path = 'Data/baseEnc.sav', woeEnc_path = 'Data/woeEnc.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weighted-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupDF_final = extra_featues_pair_data(groupDF, train, company_enc_path='Data/company_enc.sav', baseEnc_pair_path='Data/baseEnc_pair.sav', imputer_path='Data/imputer.sav', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gross-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('Data/train_data_txn.csv')  # Does not have NaNs or infs\n",
    "groupDF_final.to_csv('Data/groupDF_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "governing-great",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stand alone features\n",
      "Global features\n"
     ]
    }
   ],
   "source": [
    "groupDF_test, test_data = preprocess(test, date_path='Data/min_date.sav', baseEnc_path='Data/baseEnc.sav', woeEnc_path='Data/woeEnc.sav', train=False)\n",
    "groupDF_final_test = extra_featues_pair_data(groupDF_test, test, company_enc_path='Data/company_enc.sav', baseEnc_pair_path='Data/baseEnc_pair.sav', imputer_path='Data/imputer.sav', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "employed-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('Data/test_data_txn.csv')  # Does not have NaNs or infs\n",
    "groupDF_final_test.to_csv('Data/groupDF_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mobile-dispute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614021, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "exterior-string",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1424859, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "precious-colorado",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381971, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupDF_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "similar-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164056, 29)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupDF_final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nanCols = ['comp_dd_var_ratio',\n",
    "#  'rem_dd_var_ratio',\n",
    "#  'comp_txn_var_ratio',\n",
    "#  'rem_txn_var_ratio',\n",
    "#  'comp_txn_gap_diff',\n",
    "#  'comp_txn_gap_var_ratio',\n",
    "#  'rem_txn_gap_diff',\n",
    "#  'rem_txn_gap_var_ratio']\n",
    "\n",
    "# imputer = KNNImputer(n_neighbors=5).fit(groupDF_final[nanCols])\n",
    "# temp = imputer.transform(groupDF_final[nanCols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupDF_final[nanCols] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupDF_final.to_csv('Data/groupDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-runner",
   "metadata": {},
   "source": [
    "## inf and NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replacing inf with NaN\n",
    "\n",
    "# cols = groupDF_final.columns\n",
    "# print('Columns with infs:\\n')\n",
    "# for c in cols:\n",
    "#     if True in np.unique(np.isinf(groupDF_final.loc[:, c])):\n",
    "#         print(c)\n",
    "#         groupDF_final.loc[groupDF_final[c] == np.inf, c] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "minute-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                  :  NaN Rows Count\n",
      "n_txn                   :   0.0\n",
      "txn_amt_var             :   0.0\n",
      "txn_amt_mean            :   0.0\n",
      "txn_rate                :   0.0\n",
      "txn_gap_mean            :   0.0\n",
      "txn_gap_var             :   0.0\n",
      "txn_per_day_mean        :   0.0\n",
      "dd_mean                 :   0.0\n",
      "dd_var                  :   0.0\n",
      "comp_n_ind              :   0.0\n",
      "rem_n_ind               :   0.0\n",
      "bene_n_ind              :   0.0\n",
      "comp_dd_diff            :   0.0\n",
      "comp_dd_var_ratio       :   0.0\n",
      "rem_dd_diff             :   0.0\n",
      "rem_dd_var_ratio        :   0.0\n",
      "bene_dd_diff            :   0.0\n",
      "comp_txn_diff           :   0.0\n",
      "comp_txn_var_ratio      :   0.0\n",
      "rem_txn_diff            :   0.0\n",
      "rem_txn_var_ratio       :   0.0\n",
      "bene_txn_diff           :   0.0\n",
      "comp_txn_per_day_ratio  :   0.0\n",
      "rem_txn_per_day_ratio   :   0.0\n",
      "bene_txn_per_day_ratio  :   0.0\n",
      "comp_txn_gap_diff       :   0.0\n",
      "comp_txn_gap_var_ratio  :   0.0\n",
      "rem_txn_gap_diff        :   0.0\n",
      "rem_txn_gap_var_ratio   :   0.0\n",
      "txn_type_mean           :   0.0\n",
      "rem_company_ind_1       :   0.0\n",
      "rem_company_ind_2       :   0.0\n",
      "rem_company_ind_3       :   0.0\n",
      "rem_company_ind_4       :   0.0\n",
      "payroll_ind             :   0.0\n"
     ]
    }
   ],
   "source": [
    "wid = max([len(col) for col in groupDF_final.columns])\n",
    "col_list = []\n",
    "print('Column', (wid-6)*' ', ':  NaN Rows Count', )\n",
    "for col in groupDF_final.columns:\n",
    "    print(col, (wid-len(col))*' ', ':  ', len(groupDF_final[groupDF_final[col].isnull()])/len(groupDF_final))\n",
    "    if len(groupDF_final[groupDF_final[col].isnull()])>0:\n",
    "        col_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-quality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
